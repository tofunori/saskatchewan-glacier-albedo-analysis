"""
Fonctions utilitaires pour l'analyse des tendances d'alb√©do
==========================================================

Ce module contient toutes les fonctions utilitaires partag√©es
par les diff√©rents modules d'analyse.
"""

import numpy as np
import pandas as pd
from datetime import datetime
import warnings
import os

# Gestion des imports optionnels
try:
    import pymannkendall as mk
    PYMANNKENDALL_AVAILABLE = True
except ImportError:
    PYMANNKENDALL_AVAILABLE = False
    warnings.warn("pymannkendall non disponible. Utilisation de l'impl√©mentation manuelle.")

def check_pymannkendall():
    """
    V√©rifie si pymannkendall est disponible
    
    Returns:
        bool: True si pymannkendall est disponible
    """
    return PYMANNKENDALL_AVAILABLE

def manual_mann_kendall(data):
    """
    Impl√©mentation manuelle du test Mann-Kendall si pymannkendall n'est pas disponible
    Avec correction pour les √©galit√©s (ties) selon Hipel & McLeod 1994
    
    Args:
        data (array-like): S√©rie temporelle √† analyser
        
    Returns:
        dict: R√©sultats du test avec keys: trend, p_value, tau, s, z
    """
    data = np.array(data)
    n = len(data)
    s = 0
    
    for i in range(n-1):
        for j in range(i+1, n):
            s += np.sign(data[j] - data[i])
    
    # Correction pour les √©galit√©s (ties) - Hipel & McLeod 1994
    unique, counts = np.unique(data, return_counts=True)
    tie_term = np.sum(counts * (counts - 1) * (2*counts + 5))
    var_s = (n * (n - 1) * (2 * n + 5) - tie_term) / 18
    
    # Correction de continuit√© avec protection contre division par z√©ro
    if var_s <= 0:
        z = 0
    elif abs(s) <= 1:
        z = 0  # Pas de correction si |s| <= 1
    else:
        sign_s = np.sign(s)
        z = (s - sign_s) / np.sqrt(var_s)
    
    # Calcul de la p-value (test bilat√©ral)
    from scipy.stats import norm
    p_value = 2 * (1 - norm.cdf(abs(z))) if z != 0 else 1.0
    
    # Calcul du tau de Kendall-b (corrig√© pour les √©galit√©s)
    n_pairs = n * (n - 1) / 2
    n_ties = np.sum(counts * (counts - 1) / 2)
    denominator = n_pairs - n_ties if n_ties > 0 else n_pairs
    tau = s / denominator if denominator > 0 else 0
    
    # D√©termination de la tendance
    if p_value < 0.05:
        trend = 'increasing' if s > 0 else 'decreasing'
    else:
        trend = 'no trend'
    
    return {
        'trend': trend,
        'p_value': p_value,
        'tau': tau,
        's': s,
        'z': z
    }

def prewhiten_series(series):
    """
    Pr√©-blanchiment d'une s√©rie temporelle (suppression de la composante AR(1))
    
    Args:
        series (array-like): S√©rie temporelle √† pr√©-blanchir
        
    Returns:
        np.array: S√©rie pr√©-blanchie
    """
    series = np.array(series)
    
    # Estimation du coefficient AR(1)
    if len(series) < 2:
        return series
    
    rho = np.corrcoef(series[:-1], series[1:])[0, 1]
    
    # S√©rie pr√©-blanchie
    prewhitened = np.zeros(len(series))
    prewhitened[0] = series[0]
    
    for i in range(1, len(series)):
        prewhitened[i] = series[i] - rho * series[i-1]
    
    return prewhitened[1:]  # Enlever le premier √©l√©ment

def calculate_autocorrelation(data, lag=1):
    """
    Calcule l'autocorr√©lation d'une s√©rie temporelle
    
    Args:
        data (array-like): S√©rie temporelle
        lag (int): D√©calage pour l'autocorr√©lation
        
    Returns:
        float: Coefficient d'autocorr√©lation
    """
    data = np.array(data)
    
    if len(data) <= lag:
        return 0.0
    
    return np.corrcoef(data[:-lag], data[lag:])[0, 1]

def validate_data(data, min_obs=10):
    """
    Valide les donn√©es pour l'analyse statistique
    
    Args:
        data (array-like): Donn√©es √† valider
        min_obs (int): Nombre minimum d'observations requises
        
    Returns:
        tuple: (is_valid, cleaned_data, n_removed)
    """
    original_data = np.array(data)
    
    # Supprimer les NaN
    cleaned_data = original_data[~np.isnan(original_data)]
    n_removed = len(original_data) - len(cleaned_data)
    
    # V√©rifier le nombre minimum d'observations
    is_valid = len(cleaned_data) >= min_obs
    
    return is_valid, cleaned_data, n_removed

def format_pvalue(p_value, precision=4):
    """
    Formate une p-value pour l'affichage
    
    Args:
        p_value (float): P-value √† formater
        precision (int): Nombre de d√©cimales
        
    Returns:
        str: P-value format√©e
    """
    if p_value < 0.001:
        return "< 0.001"
    else:
        return f"{p_value:.{precision}f}"

def create_time_index(dates):
    """
    Cr√©e un index temporel d√©cimal pour les analyses de r√©gression
    Utilise une conversion pr√©cise tenant compte des ann√©es bissextiles
    
    Args:
        dates (pd.Series): S√©rie de dates
        
    Returns:
        np.array: Ann√©es d√©cimales
    """
    if isinstance(dates, pd.Series):
        dates = pd.to_datetime(dates)
        years = dates.dt.year
        day_of_year = dates.dt.dayofyear
        
        # Conversion pr√©cise avec prise en compte des ann√©es bissextiles
        # Utilise 365.2425 (moyenne gr√©gorienne) plus pr√©cise que 365.25
        is_leap_year = dates.dt.is_leap_year
        days_in_year = np.where(is_leap_year, 366, 365)
        
        # Calcul pr√©cis de la fraction d'ann√©e
        decimal_years = years + (day_of_year - 1) / days_in_year
        return decimal_years.values
    else:
        raise ValueError("Dates doit √™tre une pd.Series")

def safe_divide(numerator, denominator, default=0):
    """
    Division s√©curis√©e qui √©vite la division par z√©ro
    
    Args:
        numerator: Num√©rateur
        denominator: D√©nominateur
        default: Valeur par d√©faut si division par z√©ro
        
    Returns:
        R√©sultat de la division ou valeur par d√©faut
    """
    if denominator == 0 or np.isnan(denominator):
        return default
    return numerator / denominator

def print_section_header(title, level=1):
    """
    Affiche un en-t√™te de section format√©
    
    Args:
        title (str): Titre de la section
        level (int): Niveau d'en-t√™te (1, 2, ou 3)
    """
    if level == 1:
        print("\n" + "="*80)
        print(f"üî¨ {title.upper()}")
        print("="*80)
    elif level == 2:
        print("\n" + "-"*60)
        print(f"üìä {title}")
        print("-"*60)
    else:
        print(f"\n### {title}")

def ensure_directory_exists(path):
    """
    S'assure que le r√©pertoire existe
    
    Args:
        path (str): Chemin vers le fichier ou r√©pertoire
    """
    # Si le chemin se termine par une extension de fichier, extraire le r√©pertoire
    if os.path.splitext(path)[1]:  # A une extension de fichier
        directory = os.path.dirname(path)
    else:  # C'est d√©j√† un r√©pertoire
        directory = path
    
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

def get_timestamp():
    """
    Retourne un timestamp format√© pour les rapports
    
    Returns:
        str: Timestamp format√©
    """
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

def calculate_sen_slope(times, values):
    """
    Calcule la pente de Sen et intervalle de confiance
    Trie les donn√©es par temps pour assurer la reproductibilit√©
    
    Args:
        times (array): Temps (ann√©es d√©cimales)
        values (array): Valeurs d'alb√©do
        
    Returns:
        dict: R√©sultats de la pente de Sen
    """
    try:
        from scipy.stats import theilslopes
        
        # Convertir en arrays numpy et trier par temps pour reproductibilit√©
        times = np.array(times)
        values = np.array(values)
        
        # Trier par ordre chronologique
        sort_idx = np.argsort(times)
        times_sorted = times[sort_idx]
        values_sorted = values[sort_idx]
        
        # Calcul avec theilslopes de scipy sur donn√©es tri√©es
        slope, intercept, low_slope, high_slope = theilslopes(values_sorted, times_sorted, 0.95)
        
        # Calculer les valeurs par d√©cennie une seule fois
        slope_per_decade = slope * 10
        low_per_decade = low_slope * 10
        high_per_decade = high_slope * 10
        
        return {
            'slope': slope,
            'slope_per_decade': slope_per_decade,
            'intercept': intercept,
            'confidence_interval': {
                'low': low_slope,
                'high': high_slope,
                'low_per_decade': low_per_decade,
                'high_per_decade': high_per_decade
            },
            'method': 'theilslopes'
        }
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Erreur calcul pente Sen: {e}")
        return {
            'slope': np.nan,
            'slope_per_decade': np.nan,
            'intercept': np.nan,
            'confidence_interval': {
                'low': np.nan,
                'high': np.nan,
                'low_per_decade': np.nan,
                'high_per_decade': np.nan
            },
            'method': 'failed'
        }

def perform_mann_kendall_test(values):
    """
    Effectue le test de Mann-Kendall
    
    Args:
        values (array): Valeurs √† analyser
        
    Returns:
        dict: R√©sultats du test Mann-Kendall
    """
    if check_pymannkendall():
        try:
            import pymannkendall as mk
            mk_result = mk.original_test(values)
            return {
                'trend': mk_result.trend,
                'p_value': mk_result.p,
                'tau': mk_result.Tau,
                's': mk_result.s,
                'z': mk_result.z,
                'method': 'pymannkendall'
            }
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Erreur pymannkendall: {e}, utilisation manuelle")
            return manual_mann_kendall(values)
    else:
        return manual_mann_kendall(values)

def create_output_filename(base_name, variable, extension='png'):
    """
    Cr√©e un nom de fichier standardis√© pour les sorties
    
    Args:
        base_name (str): Nom de base du fichier
        variable (str): Variable analys√©e
        extension (str): Extension du fichier
        
    Returns:
        str: Nom de fichier complet
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return f"{base_name}_{variable}_{timestamp}.{extension}"

def load_and_validate_csv(csv_path):
    """
    Charge et valide un fichier CSV
    
    Args:
        csv_path (str): Chemin vers le fichier CSV
        
    Returns:
        pd.DataFrame: Donn√©es charg√©es et valid√©es
        
    Raises:
        FileNotFoundError: Si le fichier n'existe pas
        ValueError: Si les colonnes requises sont manquantes
    """
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Fichier non trouv√©: {csv_path}")
    
    try:
        data = pd.read_csv(csv_path)
        print(f"‚úì Fichier charg√©: {len(data)} lignes, {len(data.columns)} colonnes")
        
        # V√©rifier les colonnes essentielles
        required_cols = ['date']
        missing_cols = [col for col in required_cols if col not in data.columns]
        
        if missing_cols:
            raise ValueError(f"Colonnes requises manquantes: {missing_cols}")
        
        return data
        
    except Exception as e:
        raise ValueError(f"Erreur lors du chargement du CSV: {e}")

def print_analysis_summary(results):
    """
    Affiche un r√©sum√© des r√©sultats d'analyse
    
    Args:
        results (dict): R√©sultats des analyses
    """
    from config import TREND_SYMBOLS, get_significance_marker
    
    print_section_header("R√âSUM√â DE L'ANALYSE", level=1)
    
    if 'basic_trends' in results:
        basic = results['basic_trends']
        total = len([r for r in basic.values() if not r.get('error', False)])
        significant = len([r for r in basic.values() 
                         if not r.get('error', False) and r['mann_kendall']['p_value'] < 0.05])
        
        print(f"üìä Fractions analys√©es: {total}")
        print(f"‚úÖ Tendances significatives: {significant}")
        
        if significant > 0:
            print("\nüéØ Fractions avec tendances significatives:")
            for fraction, result in basic.items():
                if not result.get('error', False) and result['mann_kendall']['p_value'] < 0.05:
                    trend = result['mann_kendall']['trend']
                    p_val = result['mann_kendall']['p_value']
                    slope = result['sen_slope']['slope_per_decade']
                    symbol = TREND_SYMBOLS.get(trend, '‚ùì')
                    sig = get_significance_marker(p_val)
                    print(f"  {symbol} {result['label']}: {trend} {sig} ({slope:.6f}/d√©cennie)")
    
    print(f"\n‚è∞ Analyse termin√©e √†: {get_timestamp()}")
    print("="*80)